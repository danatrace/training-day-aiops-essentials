{"version":"7","defaultTimeframe":{"from":"now()-2h","to":"now()"},"defaultSegments":[],"sections":[{"id":"b433f1f1-9a3b-4d4c-8387-934eac7c83fa","type":"markdown","markdown":"### Steps to create new Training env.\n* Give yourself workflow permissions by clicking this [link](/ui/apps/dynatrace.automations/settings), that will take you to workflow authorization settings. In the settings check the checkboxes for\n  * app-engine:apps:run\n  * app-engine:functions:run \n  * app-settings:objects:read\n  * select all\n  * Click on save, ignore the warning for missing permissions and click on close\n  * Note! Each participant has to do this step before starting the training!\n* Give yourself anomaly detection permissions by clicking this [link](/ui/apps/dynatrace.davis.anomalydetection/), that will take you to the anomaly detections app:\n  * In the app click on settings in the upper right corner that will open the Authorization Settings\n  * Check Select All and save\n* Download and import both workflows from the importworkflow folder from the [training-day-aiops-essentials github repo](https://github.com/danatrace/training-day-aiops-essentials)\n* Upload and deploy the 2 workflows in the workflows app\n* Add the following entries to the [Allow List in External Requests](/ui/apps/dynatrace.settings/settings/external-requests)\n  * *.github.com\n  * *.amazonaws.com\n* Create Custom AWS SSM Templates/Runbooks in your AWS Account that you are going to use for the training (These Runbooks serve to create and delete cloudformation stacks using dynatrace workflows and are used in the setup workflows)\n  * Go to the Repo and have a look at the [cloudformation folder](https://github.com/danatrace/training-day-aiops/tree/main/cloudformation)\n  * Download the cloudformation template aws_dynatrace_use_case_install_v6.yml\n  * Create a new cloudformation stack in aws using that template\n  * This will create 2 AWS Systems Manager Runbooks DynwfCreateCloudFormationStack and DynwfDeleteCloudFormationStack\n\n* [Create Oidc Connection to aws account](https://docs.dynatrace.com/docs/analyze-explore-automate/workflows/actions/aws/aws-workflows-setup) \n  * Use the AWS Account id as connection name, this is mandatory for the training assets to work properly!\n  * The role created for the OIDC connection needs to have the following permissions: I am policies that will be attached to this role:\n    * arn:aws:iam::aws:policy/AmazonEC2FullAccess (Needed to give Dynatrace Actions the permissions to create/delete/modify Ec2 Instances)\n    * arn:aws:iam::aws:policy/AmazonS3FullAccess (Needed to give Dynatrace Actions the permissions to create/delete/modify S3 Buckets and upload and delete files)\n    * arn:aws:iam::aws:policy/AmazonSSMFullAccess (Needed to give Dynatrace Actions the permissions to create/execute/delete SSM Documents/Runbooks)\n    * arn:aws:iam::aws:policy/AWSCloudFormationFullAccess (Needed to give Dynatrace Actions the permissions to create/execute/delete Cloudformation Stacks)\n    * arn:aws:iam::aws:policy/IAMFullAccess (Needed to give Dynatrace Actions the permissions to create/delete IAM Roles)\n* [Enable Cloud Operations Connection by submitting this form](https://forms.cloud.microsoft/pages/responsepage.aspx?id=o-PrcDBbXUOdZ3cW10yhkPofqCH5wnJOpzNx9FX4T2xUQ1g2MzhXNU5SWjBFOE9JNzlITjJYMlk4Ui4u&route=shorturl) (Only needed as long as the Cloud Operations Connection for AWS isnt live yet)\n* [Create Cloud Operations Connection to AWS Account](https://docs.dynatrace.com/docs/observe/infrastructure-monitoring/cloud-platform-monitoring/clouds-app-preview/create-aws-connection)\n* Create Github Connection Object for Github Workflow Actions with your Github user, use an access token with access to public repos\n* [Put the Connection you have created for Github into the get-content task for import-training-content-from-repo workflo](/ui/apps/dynatrace.automations/workflows/b2eec95a-072a-4de6-a128-6b92a3b28256)\n* In the [import-training-content-from-repo workflow](https://zdy21321.apps.dynatrace.com/ui/apps/dynatrace.automations/workflows/b2eec95a-072a-4de6-a128-6b92a3b28256?view=draft&options=) open Workflow Options and fill in the value for \"aws_connection_name\" in the Default input for \"Workflow input and result\", this needs to be the name of the AWS OIDC connection Name (should be the AWS Account_id)\n* Run [import-training-content-from-repo workflow](/ui/apps/dynatrace.automations/workflows/b2eec95a-072a-4de6-a128-6b92a3b28256)\n* Create email array of all participant users emails and presenting trainer to enter into initiation workflows for example [\"firstname.lastname@dynatrace.com\",\"firstname.lastname@dynatrace.com\"] the array will be used to loop through creating infrastructure etc.\n* Enter the Array into the value for users in the workflow options default input for all of the following workflows:\n  * [Create infrastruture for all users](/ui/apps/dynatrace.automations/workflows/e9724e2d-c38c-4772-9997-5b6d901fd9b5?view=draft&options=)\n  * [Initiate Training for all users](/ui/apps/dynatrace.automations/workflows/f7500d3d-3cf4-45d9-ab8a-23eb976b1b3e?view=draft&options=)\n  * [Create CPu Problem for all users](/ui/apps/dynatrace.automations/workflows/3ae05e5c-d5b8-4ceb-9a5d-1130b895de7a?view=draft&options=)\n  * [Create S3 bucket problem for all users](/ui/apps/dynatrace.automations/workflows/6ffde8f4-8b90-4054-921b-44c268abed42?view=live?view=draft&options=)\n  * [create simple disk problem for all users](/ui/apps/dynatrace.automations/workflows/6690b12f-9d49-4354-8026-b6b21457b884?view=live?view=draft&options=)\n  * [create advanced disk problem for all users](/ui/apps/dynatrace.automations/workflows/f046e8d2-cfb0-4c9c-807d-021e8a790c9c?view=live?view=draft&options=)\n  * [Create Security group problem for all users](/ui/apps/dynatrace.automations/workflows/a8fb0ada-3aae-402a-bbf8-dbea5ed04726?view=draft&options=)\n  * [Delete S3 buckets](/ui/apps/dynatrace.automations/workflows/f9bf43a5-f57f-4d41-86ff-25a15ad6c447?view=draft&options=)\n  * [Delete all participant Assets ](/ui/apps/dynatrace.automations/workflows/bebde7ca-881c-4c7c-ba90-b814c9b9f3a0?view=draft&options=)\n* Get DT Access Token from Linux Dynatrace Agent install page\n* Open Workflow Options and Put Access Token into Input field for DTaccesstoken in workflow [Create infrastruture for all users](/ui/apps/dynatrace.automations/workflows/e9724e2d-c38c-4772-9997-5b6d901fd9b5?view=draft&options=)\n* Open Workflow Options and choose for which part of the training the content should be created, fill out the parameter \"Training1or2\", enter a 1 for part 1 and a 2 for part 2 [Create infrastruture for all users](/ui/apps/dynatrace.automations/workflows/e9724e2d-c38c-4772-9997-5b6d901fd9b5?view=draft&options=)\n* Trigger [Create infrastruture for all users](/ui/apps/dynatrace.automations/workflows/e9724e2d-c38c-4772-9997-5b6d901fd9b5)\n* This can take a while, wait until all training assets have been created\n* Open Workflow Options and choose for which part of the training the content should be created, fill out the parameter \"Training1or2\", enter a 1 for part 1 and a 2 for part 2 [Initiate Training for all users](/ui/apps/dynatrace.automations/workflows/f7500d3d-3cf4-45d9-ab8a-23eb976b1b3e?view=draft&options=)\n* Trigger [Initiate Training for all users](/ui/apps/dynatrace.automations/workflows/f7500d3d-3cf4-45d9-ab8a-23eb976b1b3e)\n* wait until finished and the number of Agents connected match the number of users you have created infrastructure for\n* In the table at the very bottom of the notebook called \"Training userinfo\" you can find the key and usermapping (perform1 - perform20 depending on how many participants) \n* the Notebooks and dashboards created each have one of the Keys\n* Search for that specific key in notebooks and dashboards and share each notebook and dashboard with the assinged user in \"Training userinfo\" at the bottom of the notebook\n### Delete workflows\n* [Delete S3 buckets](/ui/apps/dynatrace.automations/workflows/f9bf43a5-f57f-4d41-86ff-25a15ad6c447) Can be used to delete s3 buckets that were created for the training after the training has completed\n* [Delete all participant Assets (Dashboard, Notebooks, and tagging)](/ui/apps/dynatrace.automations/workflows/bebde7ca-881c-4c7c-ba90-b814c9b9f3a0) Deletes all participant assets, should be run right after training and before DTU account is terminated. otherwise infrastructure has to be removed manually!\n* If Tenant is terminated before Delete workflow is triggered, then the AWS assets must be deleted manaually\n* Go to S3 in AWS and search for all buckets with the name perform in them and delete them manually\n* Go to Cloudformation in AWS and search for all stacks with the name perform in them and delete them manually\n\n"},{"id":"c6890af8-ea46-4f27-8306-f32d4f9fedba","type":"markdown","markdown":"### Chaos workflows\n* [Create CPu Problem for all users](/ui/apps/dynatrace.automations/workflows/3ae05e5c-d5b8-4ceb-9a5d-1130b895de7a) (Trigger 5 - 24 hours before training start for part 1 of the training)\n* [Create S3 bucket problem for all users](/ui/apps/dynatrace.automations/workflows/6ffde8f4-8b90-4054-921b-44c268abed42?view=live) (Trigger 5 - 24 hours before training start for part 1 of the training)\n* [create simple disk problem for all users](/ui/apps/dynatrace.automations/workflows/6690b12f-9d49-4354-8026-b6b21457b884?view=live) (Trigger 5 - 24 hours before training start for part 2 of the training)\n* [create advanced disk problem for all users](/ui/apps/dynatrace.automations/workflows/f046e8d2-cfb0-4c9c-807d-021e8a790c9c?view=live)\n* [Create Security group problem for all users](/ui/apps/dynatrace.automations/workflows/a8fb0ada-3aae-402a-bbf8-dbea5ed04726?view=live&task=trigger_problem) (Trigger 5 - 24 hours before training start for part 2 of the training)\n"},{"id":"bed63acc-04ec-4fac-a3bf-ad3b60321d66","type":"markdown","markdown":"### Alert workflows\n* [AWS Security create Alert for public S3 Buckets](/ui/apps/dynatrace.automations/workflows/770b3c17-d17e-4414-9137-58801b275f71) (Activate 5 - 24 hours before training start for part 1 of the training)\n* [AWS Security create Alert for public security groups](/ui/apps/dynatrace.automations/workflows/c96bdcad-64b6-45a8-a4d1-ed247c4beac0) (Activate 5 - 24 hours before training start for part 2 of the training)\n"},{"id":"957c9897-a0e6-4273-a035-8bf9bf2dce6e","type":"function","title":"Training UserInfo","showTitle":false,"drilldownPath":[],"showInput":false,"height":208,"state":{"input":{"timeframe":{"from":"now()-2h","to":"now()"},"value":"import { executionsClient } from \"@dynatrace-sdk/client-automation\";\n\nexport default async function () {\n\n    const data1 = await executionsClient.getExecutions({\n    workflow: \"e9724e2d-c38c-4772-9997-5b6d901fd9b5\",\n    limit: 1\n\n  }); \n\n\n  const data = await executionsClient.getTaskExecutionInput({\n    executionId: data1.results[0].id,\n    id: \"initiate_training_infra\"\n\n  });\n  \n  return data;\n}"},"visualizationSettings":{"table":{"hiddenColumns":[["awsregion"],["dtaccountuid"],["DTaccesstoken"],["create_dashboards"],["dt_oauth_client_id"],["InstallOIDCconnection"],["dt_oauth_client_secret"],["AWS-EBS-remediate-low-storage","install"],["AWS-EBS-remediate-low-storage","wf_triggers_active"],["AWS-EBS-remediate-low-storage","chaos_wf_aws_instances"],["AWS-EBS-remediate-low-storage","install_chaos_workflows"],["AWS-EBS-remediate-low-storage","chaos_wf_triggers_active"],["create_workflow_groups_and_policies"],["create-s3bucket-remediation-workflows","install"],["create-s3bucket-remediation-workflows","wf_triggers_active"],["create-s3bucket-remediation-workflows","install_chaos_workflows"],["create-s3bucket-remediation-workflows","chaos_wf_triggers_active"],["create_aws_systems_manager_roles_for_ec2"],["create-dynatrace-oidc-connection-settings","install"],["create-dynatrace-oidc-connection-settings","ObjectId"],["create-dynatrace-oidc-connection-settings","dynatraceawsconnection"],["create-aws-security-group-remediation-workflows","install"],["create-aws-security-group-remediation-workflows","wf_triggers_active"],["create-aws-security-group-remediation-workflows","install_chaos_workflows"],["create-aws-security-group-remediation-workflows","chaos_wf_triggers_active"],["AmazonSSMManagedInstanceCore_instance_profile_ec2"],["AWS-Ec2-remediate-high-cpu-or-memory-consumption-by-processes","install"],["AWS-Ec2-remediate-high-cpu-or-memory-consumption-by-processes","wf_triggers_active"],["AWS-Ec2-remediate-high-cpu-or-memory-consumption-by-processes","chaos_wf_aws_instances"],["AWS-Ec2-remediate-high-cpu-or-memory-consumption-by-processes","install_chaos_workflows"],["AWS-Ec2-remediate-high-cpu-or-memory-consumption-by-processes","chaos_wf_triggers_active"]],"hideColumnsForLargeResults":false,"columnWidths":{"[\"workflow\"]":299.09}},"autoSelectVisualization":false,"chartSettings":{}},"querySettings":{"maxResultRecords":1000,"defaultScanLimitGbytes":500,"maxResultMegaBytes":1,"defaultSamplingRatio":10,"enableSampling":false},"state":"error","result":{"code":540,"value":{"error":{"code":540,"message":"Execution crashed.","details":{"logs":"TypeError: Cannot read properties of undefined (reading 'id')\n    at default (file:///script.ts:13:35)\n    at eventLoopTick (ext:core/01_core.js:177:7)\n","type":"UNCAUGHT_EXCEPTION","message":"Uncaught (in promise) TypeError: Cannot read properties of undefined (reading 'id')","details":{"sourceLine":"    executionId: data1.results[0].id,","lineNumber":13,"startColumn":35,"stack":"TypeError: Cannot read properties of undefined (reading 'id')\n    at default (file:///script.ts:13:35)\n    at eventLoopTick (ext:core/01_core.js:177:7)"}}}},"notifications":[],"dateTime":"2026-01-15T03:24:32.240Z","input":{"timeframe":{"from":"now()-2h","to":"now()"},"value":"import { executionsClient } from \"@dynatrace-sdk/client-automation\";\n\nexport default async function () {\n\n    const data1 = await executionsClient.getExecutions({\n    workflow: \"e9724e2d-c38c-4772-9997-5b6d901fd9b5\",\n    limit: 1\n\n  }); \n\n\n  const data = await executionsClient.getTaskExecutionInput({\n    executionId: data1.results[0].id,\n    id: \"initiate_training_infra\"\n\n  });\n  \n  return data;\n}"},"error":"Execution crashed."},"visualization":"table"}}]}
